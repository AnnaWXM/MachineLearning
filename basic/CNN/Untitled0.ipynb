{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPwN58MRXIFmrGBVB4r4jUv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hS1jdOhkHQPB"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","import seaborn as sns\n","import pickle "]},{"cell_type":"markdown","source":["\n","# Data pre-processing and getting to know the data\n","df_data = pd.read_csv('/Users/annnwu/Desktop/github/AIMachineLearning/ai-ml/ANN/Titanic_data.csv')\n","df_names = pd.read_csv('/Users/annnwu/Desktop/github/AIMachineLearning/ai-ml/ANN/Titanic_names.csv')\n","\n","df = pd.merge(df_data, df_names, on='id', how='inner')\n","\n","pclass = df['PClass'].value_counts()\n","\n","# Print all the Travel Classes and remove the '*' category, i.e. remove one passenger who is in this class\n","df = df[df.PClass != '*']\n","pclass = df['PClass'].value_counts()\n","\n","df['Age']=df['Age'].replace(0, df['Age'][(df['Age']>0)].mean()) # keski-ikä, nollat filteröity pois\n","\n","# Dividing the data into X and y\n","X = df.loc[:, ['PClass', 'Age', 'Gender']]\n","y = df.loc[:, ['Survived']]\n","\n","\n","# Converting categorical variables to numerical dummy variables\n","ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(drop='first'), ['Gender','PClass'])], remainder='passthrough')\n","X = np.array(ct.fit_transform(X))\n","# y = y.to_numpy()\n","\n","# Dividing the material into teaching and testing data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)\n","\n","# Scale both X_train and X_test with the scaler\n","scaler_x = StandardScaler()\n","X_train = scaler_x.fit_transform(X_train)\n","X_test = scaler_x.transform(X_test)\n","\n","# Creating and teaching a neural network\n","# build ANN\n","model = Sequential()\n","model.add(Dense(12, input_dim=X.shape[1], kernel_initializer='normal', activation='relu')) # 12 kokoinen input layer\n","model.add(Dense(8, activation='relu')) # 8 kokoinen hidden layer\n","model.add(Dense(1, activation='sigmoid')) # 1 kokoinen output layer\n","model.summary()\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_crossentropy','accuracy'])\n","history = model.fit(X_train, y_train, epochs=100, batch_size=10,  verbose=1, validation_data=(X_test,y_test))\n","    \n","# visualize training\n","print(history.history.keys())\n","# \"Loss\"\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()\n","\n","# prediction y_pred using X_test data\n","y_pred = model.predict(X_test) # prosentteina\n","y_pred_class = y_pred > 0.5 # true / false\n","\n","# Testing the neural network with test data, confusion_matrix and accuracy_score\n","from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\n","cm = confusion_matrix(y_test, y_pred_class)\n","acc = accuracy_score(y_test, y_pred_class)\n","recall = recall_score(y_test, y_pred_class)\n","precision = precision_score(y_test, y_pred_class)\n","\n","print(cm)\n","print (f'accuracy_score: {acc}')\n","print (f'recall_score: {recall}')\n","print (f'precision_score: {precision}')\n","\n","print (f'y_test: {y_test.value_counts()}')\n","\n","sns.heatmap(cm, annot=True, fmt='g')\n","plt.show()\n","\n","\n","# tallentaan malli levylle\n","model.save('titanic-model.h5')\n","\n","# save encoder to disk\n","with open('titanic-ct.pickle', 'wb') as f:\n","    pickle.dump(ct, f)\n","    \n","# save scalers to disk\n","with open('titanic-scaler_x.pickle', 'wb') as f:\n","    pickle.dump(scaler_x, f)\n","    \n","\n","\n"],"metadata":{"id":"wQs-Q0iLbbd1"}}]}